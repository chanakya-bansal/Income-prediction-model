# -*- coding: utf-8 -*-
"""Copy of CC_Recruitments_2025_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LfGHOgiquaO0FpbLCfHUw-O6kRbowd1l

#CODECHEF-VIT RECRUITMENTS 2025

#Step 0: Copy this notebook

**Guidelines**:
*   Make a copy of this notebook in your Google Drive
*   Submit the editted colab notebook as your final submission

#Step 1: Import the dataset

Guidelines: Run the code below to get the dataset
"""

import pandas as pd
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
columns = ["age", "workclass", "fnlwgt", "education", "education-num", "marital-status", "occupation",
           "relationship", "race", "sex", "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"]

data = pd.read_csv(url, names=columns, skipinitialspace=True)

# one hot encoded manually
data["income"]=data["income"].map({">50K": 1, "<=50K": 0})

"""#Step 2: Import Libraries

**Guidelines**: Import the necessary libraries
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder
from sklearn.model_selection import train_test_split as split
from tensorflow import keras
from sklearn.metrics import classification_report
regularizers = keras.regularizers

import xgboost as xgb

"""#Step 3: Perform Task

**Guidelines**: Peform the given task

**tensorflow deep learning model**
"""

data.head()

# data normalisation and one hot encoding for better performance

ct=make_column_transformer(
    (MinMaxScaler(),["age", "fnlwgt","education-num", "capital-gain", "capital-loss", "hours-per-week"]),
    (OneHotEncoder(handle_unknown="ignore"),["workclass","education","marital-status","occupation","relationship","sex","native-country"])
)


#features
X=data.drop("income",axis=1)
#label
y=data["income"]


X_train,X_test,y_train,y_test=split(X,y,test_size=0.2,random_state=42)


ct.fit(X_train)


X_train_normal=ct.transform(X_train)
X_test_normal=ct.transform(X_test)

# here we are creating a tensorflow deep learning model
tf.random.set_seed(42)

# defining the model with 4 layers containinning 128,64,32,1 neurons respectivrly
model = tf.keras.Sequential([
    tf.keras.layers.Dense(
        128, activation='relu',
        kernel_regularizer=regularizers.l2(1e-4)
    ),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Dense(
        64, activation='relu',
        kernel_regularizer=regularizers.l2(1e-4)
    ),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),

    tf.keras.layers.Dense(
        32, activation='relu',
        kernel_regularizer=regularizers.l2(1e-4)
    ),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.2),
# sigmoid activation makes it give the prediction between 0 and 1
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    loss='binary_crossentropy',
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

# stops the training early if it stops seeing improvement
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# initially the learning rate is kept high, but is decreased later to stablize the weights and biases
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='loss',
    factor=0.5,
    patience=3,
    verbose=1
)

#running thrught the data 100 times
hist = model.fit(
    X_train_normal,
    y_train,
    epochs=100,
    batch_size=1024,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

#evaulating the model on testing data
model.evaluate(X_test_normal,y_test)

#visualising performance
pd.DataFrame(hist.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs")

# performance statistics
y_preds = (model.predict(X_test_normal) > 0.5).astype(int)
print(classification_report(y_test, y_preds))

"""**XGBoost**

"""

# had to import everything again and declare everything again as tensorflow was messing up the xgboost wrapper on sklearn
import pandas as pd
import xgboost as xgb
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report


columns = ["age", "workclass", "fnlwgt", "education", "education-num",
           "marital-status", "occupation", "relationship", "race", "sex",
           "capital-gain", "capital-loss", "hours-per-week", "native-country", "income"]
data = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", names=columns, skipinitialspace=True)


data["income"] = data["income"].map({">50K": 1, "<=50K": 0})


X = data.drop("income", axis=1)
y = data["income"]

ct = make_column_transformer(
    (MinMaxScaler(), ["age", "fnlwgt", "education-num", "capital-gain", "capital-loss", "hours-per-week"]),
    (OneHotEncoder(handle_unknown="ignore"), ["workclass", "education", "marital-status",
                                               "occupation", "relationship", "sex", "native-country"])
)

X_transformed = ct.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42)

# making the model
model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
model.fit(X_train, y_train)

# prediction statistics
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test Accuracy:", accuracy)
print(classification_report(y_test, y_pred))

"""#Step 4: Results and Inferences

**Guidelines**: List out your inferences here

1) Both models achieved have similar accuracy, with the deep learning model at 87.4% and XGBoost at 87%

2)Both models have high precision 0.89 0.90 and recall (0.92 0.94) for class 0, indicating strong performance on the majority class.

3)XGBoost outperforms the deep learning model for the minority class with higher precision and a slightly better F1-score.

4)Both models have relatively lower recall on class 1, but XGBoost’s recall 0.67 is marginally higher than the deep learning model’s 0.65, meaning it misses fewer high-income cases.

5)Macro and weighted averages (F1, precision, recall) are consistently higher for XGBoost, suggesting a more balanced performance across classes.

6)XGBoost is a tree-based method, known to be robust on structured,tabular data, which is seen from its slightly better performance metrics.

7)Deep neural networks are highly flexible but tend to be more sensitive to hyperparameter choices and data preprocessing on tabular data. This sensitivity can limit their robustness compared to methods like XGBoost unless meticulously tuned.

8)Although both models perform similarly overall, the XGBoost model is recommended because it provides better minority class performance and more balanced metrics, making it more reliable for predicting higher incomes in this dataset.
"""